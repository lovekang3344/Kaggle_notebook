{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":19989,"databundleVersionId":1160143,"sourceType":"competition"},{"sourceId":52540,"sourceType":"modelInstanceVersion","modelInstanceId":44134}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-20T03:27:13.837721Z","iopub.execute_input":"2024-05-20T03:27:13.838238Z","iopub.status.idle":"2024-05-20T03:27:14.800619Z","shell.execute_reply.started":"2024-05-20T03:27:13.838204Z","shell.execute_reply":"2024-05-20T03:27:14.799816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DIR_INPUT = '/kaggle/input/global-wheat-detection'\nDIR_TRAIN = f'{DIR_INPUT}/train'\nDIR_TEST = f'{DIR_INPUT}/test'","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:14.802098Z","iopub.execute_input":"2024-05-20T03:27:14.802680Z","iopub.status.idle":"2024-05-20T03:27:14.807016Z","shell.execute_reply.started":"2024-05-20T03:27:14.802654Z","shell.execute_reply":"2024-05-20T03:27:14.806084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(f'{DIR_INPUT}/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:14.808110Z","iopub.execute_input":"2024-05-20T03:27:14.808470Z","iopub.status.idle":"2024-05-20T03:27:15.109203Z","shell.execute_reply.started":"2024-05-20T03:27:14.808439Z","shell.execute_reply":"2024-05-20T03:27:15.108202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['x', 'y', 'w', 'h']] = np.stack(df['bbox'].apply(lambda x : np.fromstring(x[1:-1], sep=',', dtype=float)))\ndf.drop(columns=['bbox'], inplace=True)\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:15.111473Z","iopub.execute_input":"2024-05-20T03:27:15.112336Z","iopub.status.idle":"2024-05-20T03:27:15.863429Z","shell.execute_reply.started":"2024-05-20T03:27:15.112287Z","shell.execute_reply":"2024-05-20T03:27:15.862476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:15.865336Z","iopub.execute_input":"2024-05-20T03:27:15.866123Z","iopub.status.idle":"2024-05-20T03:27:15.872008Z","shell.execute_reply.started":"2024-05-20T03:27:15.866087Z","shell.execute_reply":"2024-05-20T03:27:15.871075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['image_id'].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:15.873442Z","iopub.execute_input":"2024-05-20T03:27:15.873706Z","iopub.status.idle":"2024-05-20T03:27:15.898002Z","shell.execute_reply.started":"2024-05-20T03:27:15.873681Z","shell.execute_reply":"2024-05-20T03:27:15.896944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(os.listdir(DIR_TRAIN))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:16.153404Z","iopub.execute_input":"2024-05-20T03:27:16.153772Z","iopub.status.idle":"2024-05-20T03:27:16.439651Z","shell.execute_reply.started":"2024-05-20T03:27:16.153745Z","shell.execute_reply":"2024-05-20T03:27:16.438720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"上面我们可以看到我们存在一些照片是没有bbox的，也就是我们如果准备采用监督学习的话，我们只能使用`3373`个样本","metadata":{}},{"cell_type":"code","source":"image_ids = df['image_id'].unique()\ntrain_size = 0.8\nsplit_len = round(len(image_ids)*train_size)\n\ntrain_ids = image_ids[:split_len]\nvalid_ids = image_ids[split_len:]\n\ntrain = df[df['image_id'].isin(train_ids)]\nvalid = df[df['image_id'].isin(valid_ids)]\ntrain.shape, valid.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:17.058082Z","iopub.execute_input":"2024-05-20T03:27:17.059096Z","iopub.status.idle":"2024-05-20T03:27:17.121534Z","shell.execute_reply.started":"2024-05-20T03:27:17.059048Z","shell.execute_reply":"2024-05-20T03:27:17.120361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['width'].unique(), df['height'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:17.569328Z","iopub.execute_input":"2024-05-20T03:27:17.570043Z","iopub.status.idle":"2024-05-20T03:27:17.579648Z","shell.execute_reply.started":"2024-05-20T03:27:17.570009Z","shell.execute_reply":"2024-05-20T03:27:17.578702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts = df['image_id'].value_counts()\nsns.displot(counts, kde=True)\nplt.xlabel('boxes')\nplt.ylabel('images')\nplt.title('boxes distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:17.987217Z","iopub.execute_input":"2024-05-20T03:27:17.987875Z","iopub.status.idle":"2024-05-20T03:27:18.638983Z","shell.execute_reply.started":"2024-05-20T03:27:17.987838Z","shell.execute_reply":"2024-05-20T03:27:18.637978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n  figsize = (num_cols * scale, num_rows * scale)\n  _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n  axes = axes.flatten()\n  for i, (ax, img) in enumerate(zip(axes, imgs)):\n    ax.imshow(img)\n    ax.axes.get_xaxis().set_visible(False)\n    ax.axes.get_yaxis().set_visible(False)\n    if titles and len(titles) > i:\n      ax.set_title(titles[i])\n  return axes\n\ndef show_bboxes(axes, bboxes, labels=None, colors=None):\n  def _make_list(obj, default_values=None):\n    if obj is None:\n      obj = default_values\n    elif not isinstance(obj, (list, tuple)):\n      obj = [obj]\n    return obj\n\n  labels = _make_list(labels)\n  colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])\n  for i, bbox in enumerate(bboxes):\n    color = colors[i % len(colors)]\n    rect = plt.Rectangle(\n      xy=(bbox[0], bbox[1]),\n      width=bbox[2] - bbox[0],\n      height=bbox[3] - bbox[1],\n      fill=False,\n      edgecolor=color,\n      linewidth=2)\n    axes.add_patch(rect)\n    if labels and len(labels) > i:\n      text_color = 'k' if color == 'w' else 'w'\n      axes.text(rect.xy[0], rect.xy[1], labels[i], va='center',\n                ha='center', fontsize=9, color=text_color,\n                bbox=dict(facecolor=color, lw=0))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:18.641189Z","iopub.execute_input":"2024-05-20T03:27:18.641820Z","iopub.status.idle":"2024-05-20T03:27:18.655489Z","shell.execute_reply.started":"2024-05-20T03:27:18.641784Z","shell.execute_reply":"2024-05-20T03:27:18.654499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_rows, num_cols = 2, 4\nimgs = [plt.imread(f'{DIR_TRAIN}/{n}.jpg') for n in df['image_id'].unique()[:num_rows*num_cols]]\nshow_images(imgs, num_rows, num_cols, scale=4)\nplt.show()\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:18.690150Z","iopub.execute_input":"2024-05-20T03:27:18.690763Z","iopub.status.idle":"2024-05-20T03:27:20.538189Z","shell.execute_reply.started":"2024-05-20T03:27:18.690737Z","shell.execute_reply":"2024-05-20T03:27:20.537101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_rows, num_cols = 1, 2\nids = df['image_id'].unique()[:num_rows*num_cols]\nimgs = [plt.imread(f'{DIR_TRAIN}/{n}.jpg') for n in ids]\naxes = show_images(imgs, num_rows, num_cols, scale=8)\nfor ax, id in zip(axes, ids):\n  datas = df[df['image_id'] == id]\n  bboxes = [(d['x'], d['y'], d['x']+d['w'], d['y']+d['h']) for _, d in datas.iterrows()]\n  show_bboxes(ax, bboxes, labels=None, colors=['w'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:20.539853Z","iopub.execute_input":"2024-05-20T03:27:20.540191Z","iopub.status.idle":"2024-05-20T03:27:21.653344Z","shell.execute_reply.started":"2024-05-20T03:27:20.540165Z","shell.execute_reply":"2024-05-20T03:27:21.651835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 构建dataset","metadata":{}},{"cell_type":"code","source":"import cv2 as cv\nimport numpy as np\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:21.654615Z","iopub.execute_input":"2024-05-20T03:27:21.654943Z","iopub.status.idle":"2024-05-20T03:27:24.036329Z","shell.execute_reply.started":"2024-05-20T03:27:21.654893Z","shell.execute_reply":"2024-05-20T03:27:24.035272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Wheat(Dataset):\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n        \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        records = self.df[self.df['image_id'] == image_id]\n        \n        image = cv.imread(f'{self.image_dir}/{image_id}.jpg', cv.IMREAD_COLOR)\n        image = cv.cvtColor(image, cv.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        \n        boxes = records[['x', 'y', 'w', 'h']].values\n        \n        area = boxes[:, 2] * boxes[:, 3]\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        \n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.uint8)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([idx])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n#             target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n            target['boxes'] = torch.tensor(sample['bboxes'])\n#             print(target['boxes']==torch.tensor(sample['bboxes']))\n        return image, target, image_id\n        \n    def __len__(self):\n        return len(self.image_ids)\n    \n    @staticmethod\n    def get_train_transform():\n        return A.Compose([\n            A.Flip(0.5),\n            ToTensorV2(p=1.0),\n        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n    @staticmethod\n    def get_valid_transform():\n        return A.Compose([\n            ToTensorV2(p=0.1)\n        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:24.038739Z","iopub.execute_input":"2024-05-20T03:27:24.039246Z","iopub.status.idle":"2024-05-20T03:27:24.055333Z","shell.execute_reply.started":"2024-05-20T03:27:24.039217Z","shell.execute_reply":"2024-05-20T03:27:24.054296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Wheat(train, DIR_TRAIN, Wheat.get_train_transform())\nvalid_dataset = Wheat(valid, DIR_TRAIN, Wheat.get_valid_transform())","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:24.056532Z","iopub.execute_input":"2024-05-20T03:27:24.056843Z","iopub.status.idle":"2024-05-20T03:27:24.086671Z","shell.execute_reply.started":"2024-05-20T03:27:24.056819Z","shell.execute_reply":"2024-05-20T03:27:24.085807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datas = [train_dataset[i] for i in range(2)]\nimgs = [d[0].permute(1, 2, 0).numpy() for d in datas]\naxes = show_images(imgs, 1, 2, scale=8)\nfor ax, (image, target, image_id) in zip(axes, datas):\n    show_bboxes(ax, target['boxes'], labels=None, colors=['r'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:24.087839Z","iopub.execute_input":"2024-05-20T03:27:24.088146Z","iopub.status.idle":"2024-05-20T03:27:25.416712Z","shell.execute_reply.started":"2024-05-20T03:27:24.088122Z","shell.execute_reply":"2024-05-20T03:27:25.415250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 模型准备","metadata":{}},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:25.418144Z","iopub.execute_input":"2024-05-20T03:27:25.418467Z","iopub.status.idle":"2024-05-20T03:27:27.943043Z","shell.execute_reply.started":"2024-05-20T03:27:25.418441Z","shell.execute_reply":"2024-05-20T03:27:27.942113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:27.944260Z","iopub.execute_input":"2024-05-20T03:27:27.944984Z","iopub.status.idle":"2024-05-20T03:27:30.315243Z","shell.execute_reply.started":"2024-05-20T03:27:27.944954Z","shell.execute_reply":"2024-05-20T03:27:30.314176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.roi_heads.box_predictor)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:30.316432Z","iopub.execute_input":"2024-05-20T03:27:30.316716Z","iopub.status.idle":"2024-05-20T03:27:30.321772Z","shell.execute_reply.started":"2024-05-20T03:27:30.316692Z","shell.execute_reply":"2024-05-20T03:27:30.320798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:30.324550Z","iopub.execute_input":"2024-05-20T03:27:30.324794Z","iopub.status.idle":"2024-05-20T03:27:30.340707Z","shell.execute_reply.started":"2024-05-20T03:27:30.324773Z","shell.execute_reply":"2024-05-20T03:27:30.339962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.roi_heads.box_predictor)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:30.341668Z","iopub.execute_input":"2024-05-20T03:27:30.341899Z","iopub.status.idle":"2024-05-20T03:27:30.346749Z","shell.execute_reply.started":"2024-05-20T03:27:30.341879Z","shell.execute_reply":"2024-05-20T03:27:30.345897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=False,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=4,\n    shuffle=False,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:30.347831Z","iopub.execute_input":"2024-05-20T03:27:30.348142Z","iopub.status.idle":"2024-05-20T03:27:30.357550Z","shell.execute_reply.started":"2024-05-20T03:27:30.348110Z","shell.execute_reply":"2024-05-20T03:27:30.356624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\n# optimizer = torch.optim.Adam(params, lr=0.005)\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = None\nnum_epochs = 10","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:27:30.358582Z","iopub.execute_input":"2024-05-20T03:27:30.359458Z","iopub.status.idle":"2024-05-20T03:27:30.583697Z","shell.execute_reply.started":"2024-05-20T03:27:30.359432Z","shell.execute_reply":"2024-05-20T03:27:30.582733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 模型训练","metadata":{}},{"cell_type":"code","source":"import time\nfrom tqdm.notebook import tqdm as tqdm\n\nitr = 1\n\ntotal_train_loss = []\ntotal_valid_loss = []\n\nlosses_value = 0\n\nfor epoch in range(num_epochs):\n\n  start_time = time.time()\n\n  # train ------------------------------\n\n  model.train()\n  train_loss = []\n  \n  pbar = tqdm(train_data_loader, desc='let\\'s train')\n  for images, targets, image_ids in pbar:\n    \n    images = list(image.to(device) for image in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n    loss_dict = model(images, targets)\n#     print(loss_dict)\n#     break\n    losses = sum(loss for loss in loss_dict.values())\n    losses_value = losses.item()\n    train_loss.append(losses_value)   \n\n    optimizer.zero_grad()\n    losses.backward()\n    optimizer.step()\n\n    pbar.set_description(f\"Epoch: {epoch+1}, Batch: {itr}, Loss: {losses_value}\")\n    itr += 1\n#   break\n  epoch_train_loss = np.mean(train_loss)\n  total_train_loss.append(epoch_train_loss)\n\n  # update the learning rate\n  if lr_scheduler is not None:\n    lr_scheduler.step()\n\n  # valid ------------------------------\n\n  with torch.no_grad():\n    valid_loss = []\n\n    for images, targets, image_ids in valid_data_loader:\n      images = list(image.to(device) for image in images)\n      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n      loss_dict = model(images, targets)\n\n      losses = sum(loss for loss in loss_dict.values())\n      loss_value = losses.item()\n      valid_loss.append(loss_value)\n        \n  epoch_valid_loss = np.mean(valid_loss)\n  total_valid_loss.append(epoch_valid_loss)    \n  \n  # print ------------------------------\n\n  print(f\"Epoch Completed: {epoch+1}/{num_epochs}, Time: {time.time()-start_time}, \"\n        f\"Train Loss: {epoch_train_loss}, Valid Loss: {epoch_valid_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:31:17.288806Z","iopub.execute_input":"2024-05-20T03:31:17.289603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nsns.set_style(style=\"whitegrid\")\nsns.lineplot(x=range(1, len(total_train_loss)+1), y=total_train_loss, label=\"Train Loss\")\nsns.lineplot(x=range(1, len(total_train_loss)+1), y=total_valid_loss, label=\"Valid Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'fasterrcnn_resnet50_fpn.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}